{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.64s/it]\n",
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize HF items, need auth token for these\n",
    "hf_auth  = os.getenv('HASH_KEY')\n",
    "model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# Set quantization configuration to load a large model with less GPU memory\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Enable FP32 CPU offloading\n",
    ")\n",
    "\n",
    "# Load the model with quantization configuration\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the LLM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task='text-generation',\n",
    "    temperature       = 0.1, # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens    = 512, # max number of tokens to generate in the output\n",
    "    repetition_penalty= 1.1,  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a  a well-specified computational problem? A well-specified computational problem is one that has the following properties:\n",
      "\n",
      "1. **Input**: The problem takes an input, which can be thought of as a set of data or parameters.\n",
      "2. **Output**: The problem produces an output, which is a function of the input.\n",
      "3. **Computational resources**: The problem requires a finite amount of computational resources (e.g., time and memory) to solve.\n",
      "4. **Well-defined solution**: There exists a unique solution to the problem for any given input.\n",
      "\n",
      "Examples of well-specified computational problems include:\n",
      "\n",
      "* Sorting a list of numbers\n",
      "* Finding the shortest path between two points on a map\n",
      "* Solving a linear system of equations\n",
      "\n",
      "On the other hand, a poorly specified computational problem may lack one or more of these properties. For example:\n",
      "\n",
      "* \"Find the most beautiful image\" - This problem lacks a clear definition of what makes an image \"beautiful\", making it difficult to specify a solution.\n",
      "* \"Solve the mystery of the universe\" - This problem is too vague and open-ended to be considered a well-specified computational problem.\n",
      "\n",
      "In this book, we will focus on solving well-specified computational problems using algorithms and programming languages. We will explore various techniques for specifying and solving computational problems, including:\n",
      "\n",
      "* Algorithm design: We will learn how to design algorithms to solve specific computational problems.\n",
      "* Programming languages: We will use programming languages such as Python, Java, and C++ to implement algorithms and solve computational problems.\n",
      "* Data structures: We will learn about different data structures such as arrays, linked lists, trees, and graphs, and how they are used to represent and manipulate data in computational problems.\n",
      "* Computational complexity theory: We will study the theoretical foundations of computation, including the concept of computational complexity and the trade-offs between time and space efficiency.\n",
      "\n",
      "By mastering these concepts and techniques, you will be able to tackle a wide range of computational problems and develop a deeper understanding of the underlying principles of computer science.\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"What is a  a well-specified computational problem\") \n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a  a well-specified computational problem? A well-specified computational problem is one that has a clear definition, and the solution to which can be verified by a computer program. In other words, it is a problem where you can write a program that checks whether a given answer is correct or not.\n",
      "\n",
      "For example, the problem of determining whether a number is prime is a well-specified computational problem. You can define what it means for a number to be prime (it has no divisors except for 1 and itself), and then write a program that checks whether a given number satisfies this condition.\n",
      "\n",
      "On the other hand, some problems are not well-specified because they do not have a clear definition, or the solution to which cannot be verified by a computer program. For example, the problem of \"write a poem that is beautiful\" is not a well-specified computational problem, because there is no clear definition of what makes a poem beautiful, and it would be difficult to write a program that could verify whether a given poem is beautiful or not.\n",
      "\n",
      "In cryptography, we often deal with well-specified computational problems, such as factoring large numbers or computing discrete logarithms in a finite field. These problems are important because they provide the foundation for many cryptographic protocols, such as public-key encryption and digital signatures.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "answer = llm(prompt=\"What is a  a well-specified computational problem\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 doc embeddings, each with a dimensionality of 384.\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"this is one document\",\n",
    "    \"and another document\"\n",
    "]\n",
    "\n",
    "embeddings = embed_model.embed_documents(docs)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "      f\"a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the pinecone vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Connect to pinecone server'''\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "api_key = os.getenv('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "cloud  = 'aws'\n",
    "region = 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'rag-test'\n",
    "\n",
    "import time\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=len(embeddings[0]),\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag-test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 14772}},\n",
       " 'total_vector_count': 14772}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "print(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1312 pages into 14772 chunks.\n",
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "['Config', '__abstractmethods__', '__annotations__', '__class__', '__class_vars__', '__config__', '__custom_root_type__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__exclude_fields__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_validators__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__include_fields__', '__init__', '__init_subclass__', '__iter__', '__json_encoder__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__post_root_validators__', '__pre_root_validators__', '__pretty__', '__private_attributes__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__schema_cache__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__try_update_forward_refs__', '__validators__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_decompose_class', '_enforce_dict_if_root', '_get_value', '_init_private_attributes', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'metadata', 'page_content', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'type', 'update_forward_refs', 'validate']\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "DATA_PATH = 'Data/'\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH,  glob=f\"**/*{'.pdf'}\", show_progress=True, loader_cls=PyMuPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} pages into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[0]\n",
    "    print(type(chunks))\n",
    "    print(type(document))\n",
    "    print(dir(document))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                             page_content  \\\n",
      "0      interpreted as a key, 290–291 \\nto the wee has...   \n",
      "1      van Emde Boas tree, 478 \\nVar Œ c, see varianc...   \n",
      "2      upper bound, 54 \\nupper-bound property, 611, 6...   \n",
      "3      universe, 273, 1155 \\nunmatched vertex, 693, 7...   \n",
      "4      unit upper-triangular matrix, 1216 \\nunit vect...   \n",
      "...                                                  ...   \n",
      "14767  We acknowledge with gratitude the contribution...   \n",
      "14768  publisher. \\nThe MIT Press would like to thank...   \n",
      "14769  c \\n 2022 Massachusetts Institute of Technolo...   \n",
      "14770  Thomas H. Cormen \\nCharles E. Leiserson \\nRona...   \n",
      "14771        Introduction to Algorithms \\nFourth Edition   \n",
      "\n",
      "                                                metadata      type  \n",
      "0      {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "1      {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "2      {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "3      {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "4      {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "...                                                  ...       ...  \n",
      "14767  {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "14768  {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "14769  {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "14770  {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "14771  {'source': 'Data\\!Introduction.to.Algorithms.p...  Document  \n",
      "\n",
      "[14772 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(columns=['page_content', 'metadata', 'type'])\n",
    "for i in range(len(chunks)):\n",
    "    data.loc[-1] = [chunks[i].page_content, chunks[i].metadata, chunks[i].type]\n",
    "    data.index = data.index + 1\n",
    "    data = data.sort_index()\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the data into pinecone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [02:16<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "''' Index the dataset and store it(Processed in batch_size)'''\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, int(len(data)), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    batch = data.iloc[i:i_end]\n",
    "    ids = [f\"{i}\" for i, x in batch.iterrows()]\n",
    "    texts = [x['page_content'] for i, x in batch.iterrows()]\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['page_content'],\n",
    "         'source': x['metadata']['source'],\n",
    "         'title': x['metadata']['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 14772}},\n",
       " 'total_vector_count': 14772}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n",
      "<class 'list'>\n",
      "4\n",
      "page_content='theory of NP-completeness. If you can establish a problem as NP-complete, you \\nprovide good evidence for its intractability. As an engineer, you would then do \\nbetter to spend your time developing an approximation algorithm (see Chapter 35)' metadata={'source': 'Data\\\\!Introduction.to.Algorithms.pdf', 'title': ''}\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "result =retriever.invoke(\"What is a well-specified computational problem\")\n",
    "print(type(retriever))\n",
    "print(type(result))\n",
    "print(len(result))\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a well-specified computational problemtheory of NP-completeness. If you can establish a problem as NP-complete, you \n",
      "provide good evidence for its intractability. As an engineer, you would then do \n",
      "better to spend your time developing an approximation algorithm (see Chapter 35) or \n",
      "a heuristic search method (see Chapter 36) rather than trying to find an exact \n",
      "solution.\n",
      "\n",
      "NP-completeness is not the only way to show that a problem is hard. There are other \n",
      "techniques, such as showing that a problem has no known polynomial-time solution \n",
      "or that it is equivalent to another known NP-complete problem. However, NP-\n",
      "completeness provides a powerful tool for establishing the hardness of many \n",
      "problems.\n",
      "\n",
      "In this chapter, we will explore the theory of NP-completeness and learn how to \n",
      "use it to establish the hardness of problems. We will also discuss some of the \n",
      "consequences of NP-completeness, including the implications for cryptography and \n",
      "the possibility of solving NP-complete problems efficiently.\n",
      "\n",
      "### 1.2 The concept of NP-completeness\n",
      "\n",
      "The concept of NP-completeness was first introduced by Stephen Cook in his \n",
      "1971 paper \"The complexity of theorem-proving procedures.\" Cook showed that \n",
      "there exists a set of problems, now called NP-complete problems, which have the \n",
      "following properties:\n",
      "\n",
      "* They are in NP, meaning that if someone were given enough time and \n",
      "computational resources, they could verify the correctness of a proposed \n",
      "solution in polynomial time.\n",
      "* Any problem in NP can be reduced to one of these NP-complete problems \n",
      "in polynomial time. This means that if someone were able to solve one of these \n",
      "NP-complete problems efficiently, they could use that solution to solve any \n",
      "problem in NP efficiently.\n",
      "\n",
      "Cook's result showed that there exist problems that are inherently difficult to \n",
      "solve exactly, even with unlimited computational resources. This had important \n",
      "implications for the field of computer science, as it meant that there may be \n",
      "limits to what can be accomplished using computers.\n",
      "\n",
      "### 1.3 The definition of NP-completeness\n",
      "\n",
      "Formally, a problem is said to be NP-complete if it satisfies two conditions:\n",
      "\n",
      "1. It is in NP, meaning that if someone were given enough time and \n",
      "computational resources, they could verify the correctness of a proposed \n",
      "solution in polynomial time.\n",
      "2. Every problem in NP can be reduced to this problem in polynomial time.\n",
      "\n",
      "The second condition is often referred to as the \"reduction\" property. It means \n",
      "that if someone were able to solve this NP-complete problem efficiently, they \n",
      "could use that solution to solve any problem in NP efficiently.\n",
      "\n",
      "A problem is said to be NP-hard if it is at least as hard as every problem in \n",
      "NP. In other words, if someone were given a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a well-specified computational problem\"\n",
    "answer = llm(prompt=prompt + retriever.invoke(prompt)[1].page_content)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm, retrieval_qa_chat_prompt\n",
    ")\n",
    "rag_pipeline = create_retrieval_chain(retriever, combine_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "rag_answer = rag_pipeline.invoke({\"input\": \"What is a  a well-specified computational problem\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Answer any use questions based solely on the context below:\n",
      "\n",
      "<context>\n",
      "The algorithm describes a speciûc computational procedure for achieving that in- \n",
      "put/output relationship for all problem instances. \n",
      "As an example, suppose that you need to sort a sequence of numbers into mono- \n",
      "tonically increasing order. This problem arises frequently in practice and provides\n",
      "\n",
      "identical, to problems for which we do know of efûcient algorithms. Computer \n",
      "scientists are intrigued by how a small change to the problem statement can cause \n",
      "a big change to the efûciency of the best known algorithm. \n",
      "You should know about NP-complete problems because some of them arise sur-\n",
      "\n",
      "theory of NP-completeness. If you can establish a problem as NP-complete, you \n",
      "provide good evidence for its intractability. As an engineer, you would then do \n",
      "better to spend your time developing an approximation algorithm (see Chapter 35)\n",
      "\n",
      "topics most relevant to you. \n",
      "Since most of the algorithms we discuss have great practical utility, we address \n",
      "implementation concerns and other engineering issues. We often provide practical \n",
      "alternatives to the few algorithms that are primarily of theoretical interest.\n",
      "</context>\n",
      "Human: What is a  a well-specified computational problem? \n",
      "\n",
      "Answer: A well-specified computational problem is one where there exists a specific algorithm that can solve it efficiently for all possible inputs. In other words, it's a problem where we know of an efficient algorithm that can produce the desired output for every input instance. For example, sorting a sequence of numbers into monotonically increasing order is a well-specified computational problem, as we know of efficient algorithms like quicksort or mergesort that can solve this problem quickly.\n",
      "\n",
      "Note: The answer only relies on the provided context and does not require external knowledge. It's a straightforward application of the definition given in the context.\n"
     ]
    }
   ],
   "source": [
    "print(rag_answer['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
